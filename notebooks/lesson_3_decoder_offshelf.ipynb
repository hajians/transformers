{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dde47a6-d4a9-425d-9285-7a3999b9665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, DistilBertTokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers_tutorial.networks.attention_head import MultiHeadAttention, FeedForward\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73685a7-f824-4384-8cd9-164ef62b5a7e",
   "metadata": {},
   "source": [
    "# Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa0dc09-632c-4a89-804c-ed9358699ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poem_raw = pd.json_normalize(pd.read_json(\"../data/verse_202412132333.json\").iloc[:,0])\n",
    "\n",
    "def preprocess(df):\n",
    "    df_ = df.copy()\n",
    "    diacritics_pattern = r'[\\u064E\\u064F\\u0650\\u0651\\u0652\\u0640]'\n",
    "    \n",
    "    df_['text'] = df_['text'].apply(lambda x: re.sub(diacritics_pattern, '', x))\n",
    "    df_['verse_index'] = (df_['vorder']-1) // 2\n",
    "\n",
    "    df_output = (\n",
    "        df_.sort_values(\"position\", ascending=True)\n",
    "        .groupby([\"poem_id\", \"verse_index\"])[\"text\"]\n",
    "        .agg(lambda x: \" - \".join(x.tolist())\n",
    "            ).reset_index()\n",
    "    )\n",
    "    \n",
    "    # df_output['text_reverse'] = df_output['text'].apply(lambda x: \" \".join(reversed(x.strip().split(\" \"))))\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7214b37-fc3b-45e7-90b0-5f96f3f0d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = preprocess(df_poem_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7250f913-b003-4cfa-851d-1b42e4e71460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poem_id</th>\n",
       "      <th>vorder</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>همچو شاهین به هوا جلوه کنان می گذرم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>تیزرو  بالی و تازنده پری داده مرا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   poem_id  vorder  position                                 text\n",
       "2   700000       3         0  همچو شاهین به هوا جلوه کنان می گذرم\n",
       "3   700000       4         1    تیزرو  بالی و تازنده پری داده مرا"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poem_raw.iloc[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f98755-9651-4fd3-857a-933ac658e2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['همچو شاهین به هوا جلوه کنان می گذرم - تیزرو  بالی و تازنده پری داده مرا'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep[['text']].iloc[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03747ef7-9222-4854-8fad-ef68d7d0c3ab",
   "metadata": {},
   "source": [
    "# Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffe1bf0-272d-4254-822e-97dedfbc5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mitra-mir/BERT-Persian-Poetry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e09bac-fb90-491a-bfb6-f140ae82f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(df_, max_length=64):\n",
    "    return tokenizer(df_['text'].values.tolist(), \n",
    "                     padding='max_length', \n",
    "                     truncation=True, \n",
    "                     max_length=max_length, \n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "def decode_tokens(tokens_, skip_special_tokens=False):\n",
    "    decoded = tokenizer.batch_decode(tokens_, skip_special_tokens=skip_special_tokens)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23b75ac-0083-4e2e-aa77-6164fe82b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = encode_inputs(df_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aebd93-f43a-419f-8545-7ac95e2afc27",
   "metadata": {},
   "source": [
    "Make sure that tokens orders are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16b557d3-8da9-43f8-a448-df80f8c6bc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,  2164,  1112, 10880,  1923,  2595,  6618, 23051,  1924,  4479,\n",
       "          1113,   120, 12679,  1112, 11976,   623,  4685,  2097,  6037,  2218,\n",
       "          2426,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " ['[CLS]',\n",
       "  'همچ',\n",
       "  '##و',\n",
       "  'شاهین',\n",
       "  'به',\n",
       "  'هوا',\n",
       "  'جلوه',\n",
       "  'کنان',\n",
       "  'می',\n",
       "  'گذر',\n",
       "  '##م',\n",
       "  '-',\n",
       "  'تیزر',\n",
       "  '##و',\n",
       "  'بالی',\n",
       "  'و',\n",
       "  'تاز',\n",
       "  '##نده',\n",
       "  'پری',\n",
       "  'داده'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][1], decode_tokens(tokens['input_ids'][1][:20]) #, decode_tokens(tokens['input_ids'][1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802ced10-1c95-49a4-9950-bb956d05334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.zeros_like(tokens['input_ids'])\n",
    "targets[:, :-1] = tokens['input_ids'][:, 1:]\n",
    "\n",
    "tokens['label'] = targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332dfc8e-903b-4612-b9de-e7f022a52a15",
   "metadata": {},
   "source": [
    "## Generate training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f9103ba-3bb4-4e93-bb25-c151ae55bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokens = Dataset.from_dict(tokens)\n",
    "full_tokens.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86618ac-fa81-42d6-91eb-2572a3bf2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2, 3656, 6916,  ...,    0,    0,    0],\n",
       "         [   2, 2164, 1112,  ...,    0,    0,    0],\n",
       "         [   2, 2063, 6079,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   2, 9910, 2441,  ...,    0,    0,    0],\n",
       "         [   2, 5613, 8071,  ...,    0,    0,    0],\n",
       "         [   2, 2143,  607,  ...,    0,    0,    0]]),\n",
       " tensor([[ 3656,  6916,  1932,  ...,     0,     0,     0],\n",
       "         [ 2164,  1112, 10880,  ...,     0,     0,     0],\n",
       "         [ 2063,  6079,  3156,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 9910,  2441,  1932,  ...,     0,     0,     0],\n",
       "         [ 5613,  8071,  1921,  ...,     0,     0,     0],\n",
       "         [ 2143,   607, 34351,  ...,     0,     0,     0]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tokens['input_ids'], full_tokens['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce264e-3176-4f48-9615-09fcb176199d",
   "metadata": {},
   "source": [
    "## Train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d0cf6f-ab70-4a7b-b8bb-2562fcfe4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FULL_DATASET = targets.shape[0]\n",
    "TRAIN_FRAC = 0.9\n",
    "TRAIN_SIZE = int(N_FULL_DATASET * TRAIN_FRAC)\n",
    "\n",
    "SEQ_LEN = full_tokens['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "578a68a3-0519-421b-90c5-acbff9655887",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict(full_tokens[:10])\n",
    "validation_data = Dataset.from_dict(full_tokens[TRAIN_SIZE:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c45a886-2a04-4a40-a84d-2857e5f0d1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.set_format(\"pt\"), train_data.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1edf4f4a-4b29-42cb-962b-3bed00337214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((414, 4), (10, 4))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.shape, train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daf71067-2f78-400e-9528-9a14927031fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fdaed-f61f-4ff1-92eb-71feaba17935",
   "metadata": {},
   "source": [
    "# Decoder Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acd28486-c52c-4fa8-bd18-1710cc15b024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(42000, 768)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(GPT2Config())\n",
    "model.resize_token_embeddings(tokenizer.vocab_size)  # Adjust model vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5be58e1e-d837-48f8-b658-5120e05e9260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(42000, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=42000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bc60b52-43e3-4d86-a2ec-ac28743869f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _ = model(**{k: v for k,v in train_data[:2].items() if k in {\"attention_mask\", \"input_ids\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95e4b2fd-4edb-43bf-ab0a-7ce29ce0848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 42000])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bde52-78f1-4ef7-904a-31628daf66ff",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11749da4-c450-4d68-acf2-4fa6798cd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b62c867-e749-45b7-85c0-ce323fe50dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    logging_steps=10, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c54b19f-6172-4b25-916d-f12999c0d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_382937/2586558677.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf41d8e9-6919-4c5a-9ce1-678029e64074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.240806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=5.688831965128581, metrics={'train_runtime': 47.1551, 'train_samples_per_second': 0.212, 'train_steps_per_second': 0.064, 'total_flos': 326615040000.0, 'train_loss': 5.688831965128581, 'epoch': 1.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f9a9a-e69b-4fd4-bd71-221bce2cd9ce",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd22c5b-637a-4e26-8b5b-c92316a4cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model_, tokenizer_, seq_len_, initial_text, max_length = 100):\n",
    "    output = initial_text.split(\" \")\n",
    "    print(initial_text, end=\" \")\n",
    "\n",
    "    for i in range(0, min(seq_len_, max_length)):\n",
    "        current_text = \" \".join(output)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tokens_ = tokenizer_(current_text, padding='max_length', truncation=True, max_length=seq_len_, return_tensors=\"pt\")\n",
    "\n",
    "            chosen_token = torch.softmax(model_(**tokens_).logits, dim=-1)[:, i, :].argmax()\n",
    "            next_word = tokenizer_.decode(chosen_token, skip_special_tokens=False)\n",
    "            print(next_word, end=\" \")\n",
    "            output += next_word\n",
    "            \n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780884e-1fe3-4ade-911b-5d291ecefba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = generate_text(model, tokenizer, SEQ_LEN, \"همچو\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ff40b-bdea-443a-bcd0-3b6975ce64e7",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf627150-f011-4c27-8776-0f2e7deb66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(f'./trained_model_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece2a5d-424e-4185-9b64-abbd3c230228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-py311",
   "language": "python",
   "name": "transformers-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
