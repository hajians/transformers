{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dde47a6-d4a9-425d-9285-7a3999b9665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, DistilBertTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers_tutorial.networks.attention_head import MultiHeadAttention, FeedForward\n",
    "from transformers_tutorial.networks.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73685a7-f824-4384-8cd9-164ef62b5a7e",
   "metadata": {},
   "source": [
    "# Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa0dc09-632c-4a89-804c-ed9358699ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poem_raw = pd.json_normalize(pd.read_json(\"../data/verse_202412132333.json\").iloc[:,0])\n",
    "\n",
    "def preprocess(df):\n",
    "    df_ = df.copy()\n",
    "    diacritics_pattern = r'[\\u064E\\u064F\\u0650\\u0651\\u0652\\u0640]'\n",
    "    \n",
    "    df_['text'] = df_['text'].apply(lambda x: re.sub(diacritics_pattern, '', x))\n",
    "    df_['verse_index'] = (df_['vorder']-1) // 2\n",
    "\n",
    "    df_output = (\n",
    "        df_.sort_values(\"position\", ascending=True)\n",
    "        .groupby([\"poem_id\", \"verse_index\"])[\"text\"]\n",
    "        .agg(lambda x: \" - \".join(x.tolist())\n",
    "            ).reset_index()\n",
    "    )\n",
    "    \n",
    "    # df_output['text_reverse'] = df_output['text'].apply(lambda x: \" \".join(reversed(x.strip().split(\" \"))))\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7214b37-fc3b-45e7-90b0-5f96f3f0d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = preprocess(df_poem_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7250f913-b003-4cfa-851d-1b42e4e71460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poem_id</th>\n",
       "      <th>vorder</th>\n",
       "      <th>position</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>همچو شاهین به هوا جلوه کنان می گذرم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>تیزرو  بالی و تازنده پری داده مرا</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   poem_id  vorder  position                                 text\n",
       "2   700000       3         0  همچو شاهین به هوا جلوه کنان می گذرم\n",
       "3   700000       4         1    تیزرو  بالی و تازنده پری داده مرا"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poem_raw.iloc[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f98755-9651-4fd3-857a-933ac658e2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['همچو شاهین به هوا جلوه کنان می گذرم - تیزرو  بالی و تازنده پری داده مرا'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep[['text']].iloc[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03747ef7-9222-4854-8fad-ef68d7d0c3ab",
   "metadata": {},
   "source": [
    "# Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffe1bf0-272d-4254-822e-97dedfbc5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mitra-mir/BERT-Persian-Poetry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e09bac-fb90-491a-bfb6-f140ae82f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(df_):\n",
    "    return tokenizer(df_['text'].values.tolist(), padding=False)\n",
    "\n",
    "def decode_tokens(tokens_, skip_special_tokens=False):\n",
    "    decoded = tokenizer.batch_decode(tokens_, skip_special_tokens=skip_special_tokens)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23b75ac-0083-4e2e-aa77-6164fe82b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = encode_inputs(df_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aebd93-f43a-419f-8545-7ac95e2afc27",
   "metadata": {},
   "source": [
    "Make sure that tokens orders are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16b557d3-8da9-43f8-a448-df80f8c6bc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2164, 1112, 10880, 1923],\n",
       " ['[CLS]',\n",
       "  'همچ',\n",
       "  '##و',\n",
       "  'شاهین',\n",
       "  'به',\n",
       "  'هوا',\n",
       "  'جلوه',\n",
       "  'کنان',\n",
       "  'می',\n",
       "  'گذر',\n",
       "  '##م',\n",
       "  '-',\n",
       "  'تیزر',\n",
       "  '##و',\n",
       "  'بالی',\n",
       "  'و',\n",
       "  'تاز',\n",
       "  '##نده',\n",
       "  'پری',\n",
       "  'داده'],\n",
       " ['[CLS] همچو شاهین به هوا جلوه کنان می گذرم - تیزرو بالی و تازنده پری داده مرا [SEP]'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][1][:5], decode_tokens(tokens['input_ids'][1][:20]), decode_tokens(tokens['input_ids'][1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332dfc8e-903b-4612-b9de-e7f022a52a15",
   "metadata": {},
   "source": [
    "## Generate training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42c62a6-1e35-4e7b-8410-f81a9fd5db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(tokens_):\n",
    "    output = []\n",
    "    target = []\n",
    "    for seq in tokens_['input_ids']:\n",
    "        for ix in range(1, len(seq)):\n",
    "            output += [torch.tensor(seq[:ix])]\n",
    "            target.append(seq[ix])\n",
    "\n",
    "    padded_tensor = pad_sequence(output, batch_first=True, padding_value=0)\n",
    "    attention_mask_tensor = (padded_tensor != 0).int()\n",
    "          \n",
    "    return {\"input_ids\": padded_tensor, \"attention_mask\": attention_mask_tensor}, torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1a8f49-72b4-4572-8f6e-982b1b4b883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokens, targets = generate_sequences(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fefdce1-d664-4800-9317-1e60077d0f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([71097, 39])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61204331-0f25-4435-92d4-be43ccc6baee",
   "metadata": {},
   "source": [
    "Check if target is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86618ac-fa81-42d6-91eb-2572a3bf2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['',\n",
       "  'خواب',\n",
       "  'خواب دیدم',\n",
       "  'خواب دیدم که',\n",
       "  'خواب دیدم که خدا',\n",
       "  'خواب دیدم که خدا بال',\n",
       "  'خواب دیدم که خدا بال و',\n",
       "  'خواب دیدم که خدا بال و پری',\n",
       "  'خواب دیدم که خدا بال و پری داده',\n",
       "  'خواب دیدم که خدا بال و پری داده مرا'],\n",
       " ['خواب',\n",
       "  'دیدم',\n",
       "  'که',\n",
       "  'خدا',\n",
       "  'بال',\n",
       "  'و',\n",
       "  'پری',\n",
       "  'داده',\n",
       "  'مرا',\n",
       "  '-',\n",
       "  'در',\n",
       "  'هوا',\n",
       "  'قوت',\n",
       "  'سیر',\n",
       "  'و',\n",
       "  'سفری',\n",
       "  'داده',\n",
       "  'مرا',\n",
       "  '',\n",
       "  'همچ'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_tokens(full_tokens['input_ids'][:10], skip_special_tokens=True), decode_tokens(targets[:20], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeefcf95-83c7-4593-a941-bdeca60db854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tokens['attention_mask'][:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce264e-3176-4f48-9615-09fcb176199d",
   "metadata": {},
   "source": [
    "## Train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d0cf6f-ab70-4a7b-b8bb-2562fcfe4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FULL_DATASET = targets.shape[0]\n",
    "TRAIN_FRAC = 0.9\n",
    "TRAIN_SIZE = int(N_FULL_DATASET * TRAIN_FRAC)\n",
    "\n",
    "SEQ_LEN = full_tokens['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578a68a3-0519-421b-90c5-acbff9655887",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({key: val[:TRAIN_SIZE] for key, val in full_tokens.items()}).add_column(\"label\", targets[:TRAIN_SIZE].numpy())\n",
    "validation_data = Dataset.from_dict({key: val[TRAIN_SIZE:] for key, val in full_tokens.items()}).add_column(\"label\", targets[TRAIN_SIZE:].numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c45a886-2a04-4a40-a84d-2857e5f0d1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.set_format(\"pt\"), train_data.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1edf4f4a-4b29-42cb-962b-3bed00337214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7110, 3), (63987, 3))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.shape, train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fdaed-f61f-4ff1-92eb-71feaba17935",
   "metadata": {},
   "source": [
    "# Decoder Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2db8dab0-fccd-44c0-8a61-c61e05faa859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_heads, intermediate_dim, device, p_dropout=0.2, seq_len=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        config.vocab_size = vocab_size\n",
    "        config.hidden_dropout_prob = p_dropout\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        if seq_len:\n",
    "            config.max_position_embeddings = seq_len\n",
    "            \n",
    "        self.embeddings = AutoModel.from_config(config).embeddings\n",
    "\n",
    "        hidden_dim = config.hidden_size\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            emb_dim=config.hidden_size, hidden_dim=hidden_dim, n_heads=n_heads, is_decoder=True,\n",
    "        )\n",
    "        self.ff = FeedForward(\n",
    "            hidden_dim=hidden_dim,\n",
    "            intermediate_dim=intermediate_dim,\n",
    "            p_dropout=p_dropout,\n",
    "        )\n",
    "        self.layer_norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_):\n",
    "        data = {\n",
    "            k: input_[k].to(self.device)\n",
    "            for k in input_.keys()\n",
    "            if k in [\"attention_mask\", \"input_ids\"]\n",
    "        }\n",
    "\n",
    "        x = self.embeddings(data['input_ids'])\n",
    "        \n",
    "        residual = x\n",
    "        \n",
    "        x = residual + self.multi_head_attention(x, data[\"attention_mask\"])\n",
    "        x = self.layer_norm_1(x)\n",
    "\n",
    "        residual = x\n",
    "        x = residual + self.ff(x)\n",
    "        x = self.layer_norm_2(x)       \n",
    "        \n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        # Use hidden layer corresponding to last non [PAD] token.\n",
    "        \n",
    "        last_non_padded = data['attention_mask'].sum(dim=1) - 1 # To get index\n",
    "        batch_size = last_non_padded.shape[0]\n",
    "        return logits[torch.arange(batch_size), last_non_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "623be110-8d4b-4197-8d5e-3a4dcf238897",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ea8d43fa-2fd8-479e-b4eb-bfdf32776115",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "INTERMEDIATE_DIM = 512 * 4\n",
    "N_HEADS = 12\n",
    "\n",
    "trasnformer_decoder = TransformerDecoderLayer(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    # hidden_dim=HIDDEN_DIM, \n",
    "    n_heads=N_HEADS, \n",
    "    intermediate_dim=INTERMEDIATE_DIM,\n",
    "    seq_len=SEQ_LEN,\n",
    "    device=device,\n",
    "    p_dropout=0.1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4e1985e3-1353-4936-82e2-c45e4b4d8a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoderLayer(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(39, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (multi_head_attention): MultiHeadAttention(\n",
       "    (heads): ModuleList(\n",
       "      (0-11): 12 x AttentionHead(\n",
       "        (q): Linear(in_features=768, out_features=64, bias=True)\n",
       "        (k): Linear(in_features=768, out_features=64, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (layers): Sequential(\n",
       "      (layer_1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (layer_2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=768, out_features=42000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trasnformer_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a8d13-fce2-45ed-a199-b42b40ee6413",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "913e62b3-a51b-4864-87fa-9d71d356bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in trasnformer_decoder.named_parameters():\n",
    "#     if i[1].requires_grad:\n",
    "#         print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "585b8fd5-4fd7-465e-bd15-ca0933e10e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['دینامو', '##قهر']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    _ = trasnformer_decoder(validation_data[:2])\n",
    "\n",
    "decode_tokens(torch.softmax(_, dim=-1).argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "85464217-cc99-412f-953b-5150a2dfeb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params={p for p in trasnformer_decoder.parameters() if p.requires_grad}, \n",
    "    lr=1e-5, weight_decay=0.01\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "trainer = Trainer(optimizer=optimizer, loss=loss_fn, model=trasnformer_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88210977-0e0c-4a38-88d3-3d0335de8471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|█▍                             | 6/128 [00:04<01:26,  1.41batch/s]"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 1\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "_ = trainer.train(train_data=train_data.select(range(0, 2048)), \n",
    "                  validation_data=validation_data.select(range(0, 1024)), \n",
    "                  n_epochs=N_EPOCHS,\n",
    "                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5a08b35f-74ae-4483-89b9-39b0d18b4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model_, tokenizer_, initial_text, max_length = 100):\n",
    "    output = initial_text.split(\" \")\n",
    "    \n",
    "    for _ in range(0, min(model_.seq_len, max_length)):\n",
    "        current_text = \" \".join(output)\n",
    "        print(initial_text, end=\"\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tokens_ = tokenizer_(current_text, padding='max_length', truncation=True, max_length=model_.seq_len, return_tensors=\"pt\")\n",
    "\n",
    "            chosen_token = torch.softmax(model_(tokens_), dim=-1).argmax()\n",
    "            next_word = tokenizer_.decode(chosen_token, skip_special_tokens=True)\n",
    "            print(next_word, end=\"\")\n",
    "            output += next_word\n",
    "            \n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "007f1eec-af97-4104-9140-a0f9a30a00d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ابروابروابروابروابروابر-ابر-ابر-ابر-ابروابروابروابروابروابروابرابرابروابرابرابرابرابرابرابرابرابرابروابرابرابرابرابرابرابرابرابرابرابر"
     ]
    }
   ],
   "source": [
    "_ = generate_text(trasnformer_decoder, tokenizer, \"ابر\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-py311",
   "language": "python",
   "name": "transformers-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
